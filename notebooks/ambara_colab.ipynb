{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ambara — Colab Runner\n",
    "\n",
    "Automated notebook for running **clip extraction** and **ASR training** on\n",
    "Colab with Google Drive persistence.\n",
    "\n",
    "**Usage:** Edit the configuration cell below, then **Runtime > Run All**.\n",
    "\n",
    "- Repo is cloned to the VM for fast I/O.\n",
    "- `data/` and `models/` are symlinked to Google Drive so they persist across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Google Drive ----\n",
    "DRIVE_ROOT = \"ambara\"\n",
    "\n",
    "# ---- Repository ----\n",
    "REPO_URL = \"https://github.com/ny-randriantsarafara/ny-feoko.git\"\n",
    "REPO_BRANCH = \"main\"\n",
    "\n",
    "# ---- HuggingFace (leave empty to skip login) ----\n",
    "HF_TOKEN = \"\"\n",
    "\n",
    "# ---- Clip Extraction (set EXTRACT_ENABLED = False to skip) ----\n",
    "EXTRACT_ENABLED = True\n",
    "EXTRACT_INPUT = \"data/input/my-recording.wav\"\n",
    "EXTRACT_WHISPER_MODEL = \"small\"\n",
    "EXTRACT_WHISPER_HF = \"\"  # HuggingFace model ID — overrides EXTRACT_WHISPER_MODEL\n",
    "EXTRACT_VAD_THRESHOLD = 0.35\n",
    "EXTRACT_SPEECH_THRESHOLD = 0.35\n",
    "EXTRACT_LABEL = \"\"\n",
    "\n",
    "# ---- ASR Training (set TRAIN_ENABLED = False to skip) ----\n",
    "TRAIN_ENABLED = True\n",
    "TRAIN_DATASET = \"data/training/my-dataset\"\n",
    "TRAIN_BASE_MODEL = \"openai/whisper-small\"\n",
    "TRAIN_OUTPUT_DIR = \"models/whisper-mg-v1\"\n",
    "TRAIN_EPOCHS = 10\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "TRAIN_LR = 1e-5\n",
    "TRAIN_PUSH_TO_HUB = \"\"  # HuggingFace repo ID — leave empty to skip\n",
    "\n",
    "# ---- Re-draft (set REDRAFT_ENABLED = False to skip) ----\n",
    "REDRAFT_ENABLED = True\n",
    "REDRAFT_RUN_DIR = \"data/output/my-run\"  # local directory with clips/\n",
    "REDRAFT_LABEL = \"\"  # run label in Supabase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Mounts Drive, clones the repo, creates symlinks for `data/` and `models/`,\n",
    "and installs Python dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "DRIVE_MOUNT = Path(\"/content/drive\")\n",
    "DRIVE_BASE = DRIVE_MOUNT / \"MyDrive\" / DRIVE_ROOT\n",
    "REPO_DIR = Path(\"/content/ny-feoko\")\n",
    "\n",
    "# ---- Mount Google Drive ----\n",
    "if not (DRIVE_MOUNT / \"MyDrive\").exists():\n",
    "    drive.mount(str(DRIVE_MOUNT))\n",
    "print(f\"Drive mounted at {DRIVE_MOUNT}\")\n",
    "\n",
    "# ---- Create Drive directories ----\n",
    "for subdir in [\"data/input\", \"data/output\", \"data/training\", \"models\"]:\n",
    "    (DRIVE_BASE / subdir).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Drive root: {DRIVE_BASE}\")\n",
    "\n",
    "# ---- Clone or update repo ----\n",
    "if REPO_DIR.exists():\n",
    "    subprocess.run(\n",
    "        [\"git\", \"-C\", str(REPO_DIR), \"pull\", \"--ff-only\"],\n",
    "        check=True,\n",
    "    )\n",
    "    print(f\"Repo updated: {REPO_DIR}\")\n",
    "else:\n",
    "    subprocess.run(\n",
    "        [\"git\", \"clone\", \"-b\", REPO_BRANCH, REPO_URL, str(REPO_DIR)],\n",
    "        check=True,\n",
    "    )\n",
    "    print(f\"Repo cloned: {REPO_DIR}\")\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "\n",
    "# ---- Symlink data/ and models/ to Drive ----\n",
    "for name in [\"data\", \"models\"]:\n",
    "    link = REPO_DIR / name\n",
    "    target = DRIVE_BASE / name\n",
    "    if link.is_symlink():\n",
    "        link.unlink()\n",
    "    elif link.is_dir():\n",
    "        shutil.rmtree(link)\n",
    "    link.symlink_to(target)\n",
    "    print(f\"  {name}/ -> {target}\")\n",
    "\n",
    "# ---- Symlink .env from Drive (if present) ----\n",
    "env_drive = DRIVE_BASE / \".env\"\n",
    "env_local = REPO_DIR / \".env\"\n",
    "if env_drive.exists():\n",
    "    if env_local.is_symlink() or env_local.exists():\n",
    "        env_local.unlink()\n",
    "    env_local.symlink_to(env_drive)\n",
    "    print(f\"  .env -> {env_drive}\")\n",
    "\n",
    "# ---- Python version check ----\n",
    "v = sys.version_info\n",
    "print(f\"\\nPython {v.major}.{v.minor}.{v.micro}\")\n",
    "if v < (3, 10):\n",
    "    print(\"WARNING: This project requires Python >= 3.10.\")\n",
    "\n",
    "# ---- Install dependencies ----\n",
    "subprocess.run([\"make\", \"colab-install\"], check=True, cwd=str(REPO_DIR))\n",
    "\n",
    "# ---- HuggingFace login ----\n",
    "if HF_TOKEN:\n",
    "    from huggingface_hub import login\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"Logged in to HuggingFace Hub.\")\n",
    "\n",
    "# ---- Environment summary ----\n",
    "import torch\n",
    "\n",
    "gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA:    {torch.cuda.is_available()} ({gpu_name})\")\n",
    "print(f\"Drive:   {DRIVE_BASE}\")\n",
    "print(f\"Repo:    {REPO_DIR}\")\n",
    "print(f\"{'=' * 50}\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip Extraction\n",
    "\n",
    "Runs the full pipeline: **VAD → classify → transcribe → write clips**.\n",
    "\n",
    "Input file must exist at the configured path on Drive (e.g.\n",
    "`My Drive/ambara/data/input/my-recording.wav`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not EXTRACT_ENABLED:\n",
    "    print(\"Clip extraction skipped (EXTRACT_ENABLED = False).\")\n",
    "else:\n",
    "    cmd = [\n",
    "        \"python\", \"-m\", \"clip_extraction.cli\", \"run\",\n",
    "        \"--input\", EXTRACT_INPUT,\n",
    "        \"--output\", \"data/output\",\n",
    "        \"--device\", \"cuda\",\n",
    "        \"--vad-threshold\", str(EXTRACT_VAD_THRESHOLD),\n",
    "        \"--speech-threshold\", str(EXTRACT_SPEECH_THRESHOLD),\n",
    "        \"--verbose\",\n",
    "    ]\n",
    "    if EXTRACT_WHISPER_HF:\n",
    "        cmd += [\"--whisper-hf\", EXTRACT_WHISPER_HF]\n",
    "    else:\n",
    "        cmd += [\"--whisper-model\", EXTRACT_WHISPER_MODEL]\n",
    "    if EXTRACT_LABEL:\n",
    "        cmd += [\"--label\", EXTRACT_LABEL]\n",
    "\n",
    "    print(f\"Running: {' '.join(cmd)}\")\n",
    "    subprocess.run(cmd, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASR Training\n",
    "\n",
    "Fine-tunes Whisper on the configured training dataset. The dataset must\n",
    "already exist on Drive (exported locally via `./ambara export-training`,\n",
    "then uploaded to `My Drive/ambara/data/training/<dataset>/`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN_ENABLED:\n",
    "    print(\"ASR training skipped (TRAIN_ENABLED = False).\")\n",
    "else:\n",
    "    cmd = [\n",
    "        \"python\", \"-m\", \"asr_training.cli\", \"train\",\n",
    "        \"--data-dir\", TRAIN_DATASET,\n",
    "        \"--output-dir\", TRAIN_OUTPUT_DIR,\n",
    "        \"--device\", \"cuda\",\n",
    "        \"--base-model\", TRAIN_BASE_MODEL,\n",
    "        \"--epochs\", str(TRAIN_EPOCHS),\n",
    "        \"--batch-size\", str(TRAIN_BATCH_SIZE),\n",
    "        \"--lr\", str(TRAIN_LR),\n",
    "    ]\n",
    "    if TRAIN_PUSH_TO_HUB:\n",
    "        cmd += [\"--push-to-hub\", TRAIN_PUSH_TO_HUB]\n",
    "\n",
    "    print(f\"Running: {' '.join(cmd)}\")\n",
    "    subprocess.run(cmd, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-draft Pending Clips\n",
    "\n",
    "Uses the trained model to re-transcribe pending clips in Supabase.\n",
    "Only clips with `status = 'pending'` are updated — corrected clips are left untouched.\n",
    "\n",
    "Requires `.env` with Supabase credentials on Drive (`My Drive/ambara/.env`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not REDRAFT_ENABLED:\n",
    "    print(\"Re-draft skipped (REDRAFT_ENABLED = False).\")\n",
    "else:\n",
    "    # Use the trained model (local or HuggingFace)\n",
    "    model_path = TRAIN_PUSH_TO_HUB if TRAIN_PUSH_TO_HUB else f\"{TRAIN_OUTPUT_DIR}/model\"\n",
    "    \n",
    "    cmd = [\n",
    "        \"python\", \"-m\", \"asr_training.cli\", \"re-draft\",\n",
    "        \"--model\", model_path,\n",
    "        \"-d\", REDRAFT_RUN_DIR,\n",
    "        \"--device\", \"cuda\",\n",
    "    ]\n",
    "    if REDRAFT_LABEL:\n",
    "        cmd += [\"--label\", REDRAFT_LABEL]\n",
    "\n",
    "    print(f\"Running: {' '.join(cmd)}\")\n",
    "    subprocess.run(cmd, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Shows extraction and training outputs, Drive usage, and suggested next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'=' * 50}\")\n",
    "print(\"Results\")\n",
    "print(f\"{'=' * 50}\")\n",
    "\n",
    "# ---- Extraction output ----\n",
    "output_dir = REPO_DIR / \"data\" / \"output\"\n",
    "if output_dir.exists():\n",
    "    runs = sorted(d for d in output_dir.iterdir() if d.is_dir())\n",
    "    if runs:\n",
    "        latest = runs[-1]\n",
    "        clips_dir = latest / \"clips\"\n",
    "        clip_count = len(list(clips_dir.glob(\"*.wav\"))) if clips_dir.exists() else 0\n",
    "        print(f\"\\nLatest extraction run: {latest.name}\")\n",
    "        print(f\"  Clips extracted: {clip_count}\")\n",
    "\n",
    "# ---- Training output ----\n",
    "model_dir = REPO_DIR / TRAIN_OUTPUT_DIR / \"model\"\n",
    "if model_dir.exists():\n",
    "    size_mb = sum(f.stat().st_size for f in model_dir.rglob(\"*\") if f.is_file()) / (1024 * 1024)\n",
    "    print(f\"\\nTrained model: {model_dir}\")\n",
    "    print(f\"  Size: {size_mb:.0f} MB\")\n",
    "    print(f\"  Persisted at: {DRIVE_BASE / TRAIN_OUTPUT_DIR / 'model'}\")\n",
    "\n",
    "if TRAIN_PUSH_TO_HUB:\n",
    "    print(f\"  HuggingFace: https://huggingface.co/{TRAIN_PUSH_TO_HUB}\")\n",
    "\n",
    "# ---- Drive usage ----\n",
    "print(f\"\\nDrive usage ({DRIVE_BASE}):\")\n",
    "for subdir in [\"data/input\", \"data/output\", \"data/training\", \"models\"]:\n",
    "    p = DRIVE_BASE / subdir\n",
    "    if p.exists():\n",
    "        total = sum(f.stat().st_size for f in p.rglob(\"*\") if f.is_file())\n",
    "        print(f\"  {subdir}: {total / (1024 * 1024):.0f} MB\")\n",
    "\n",
    "# ---- Next steps ----\n",
    "next_model = TRAIN_PUSH_TO_HUB if TRAIN_PUSH_TO_HUB else f\"{TRAIN_OUTPUT_DIR}/model\"\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  ./ambara re-draft --model {next_model} \\\\\")\n",
    "print(f\"      -d data/output/<run-dir> --label <run-label>\")\n",
    "if TRAIN_PUSH_TO_HUB:\n",
    "    print(f\"  ./ambara extract -i audio.wav -o data/output/ --device mps \\\\\")\n",
    "    print(f\"      --whisper-hf {TRAIN_PUSH_TO_HUB}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ambara_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
